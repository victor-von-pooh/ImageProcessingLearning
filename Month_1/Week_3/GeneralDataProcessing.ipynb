{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18b36f97",
   "metadata": {},
   "source": [
    "# 一般的な画像データの処理\n",
    "今までは, PyTorch が提供するベンチマークデータセットを取り扱っていた.  \n",
    "今回は普通の画像データ(拡張子は `.png`)を用いた一般的な画像処理のやり方を学習する.  \n",
    "まずは必要なライブラリをインポートする."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3395a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 必要なライブラリのインポート\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147078f6",
   "metadata": {},
   "source": [
    "ノートブック全体で使う関数を定義する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf02c849",
   "metadata": {},
   "outputs": [],
   "source": [
    "def outputs_setter(model: nn.Module, dataloader: DataLoader) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    モデルの出力を取得する関数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model: nn.Module\n",
    "        学習済みモデル\n",
    "    inputs: torch.Tensor\n",
    "        入力データ\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    outputs: torch.Tensor\n",
    "        モデルの出力データ\n",
    "    \"\"\"\n",
    "    # モデルを評価モードに設定\n",
    "    model.eval()\n",
    "\n",
    "    # 勾配計算を無効化\n",
    "    with torch.no_grad():\n",
    "        # モデルの出力を取得\n",
    "        images, _ = next(iter(dataloader))\n",
    "        outputs = model(images)\n",
    "\n",
    "    return outputs\n",
    "\n",
    "\n",
    "def plot_data(train_loss_list: list) -> None:\n",
    "    \"\"\"\n",
    "    学習過程の損失をプロットする関数\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train_loss_list: list\n",
    "        学習過程の損失を格納したリスト\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # 横軸の設定\n",
    "    x = [i + 1 for i in range(len(train_loss_list))]\n",
    "\n",
    "    # 出力画像の設定\n",
    "    plt.figure(figsize=(18, 12), tight_layout=True)\n",
    "    plt.title(\"Training Loss over Epochs\", size=15, color=\"red\")\n",
    "    plt.grid()\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "\n",
    "    # 学習過程の損失をプロット\n",
    "    plt.plot(x, train_loss_list)\n",
    "\n",
    "    # グラフの表示\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b44b8e",
   "metadata": {},
   "source": [
    "画像データを適切なディレクトリに配置し, `ImageFolder` を使うことで簡単にデータセットに変換できる."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f72476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 乱数シードの設定\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# 画像の前処理の定義\n",
    "data_transform = transforms.Compose(\n",
    "    [transforms.Resize((256, 256)), transforms.ToTensor()]\n",
    ")\n",
    "\n",
    "# 画像データセットの読み込み\n",
    "batch_size = 64\n",
    "dataset = ImageFolder(\"./\", transform=data_transform)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# データの確認\n",
    "for images, _ in dataloader:\n",
    "    plt.imshow(images[0].permute(1, 2, 0))\n",
    "    plt.show()\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8b55bd",
   "metadata": {},
   "source": [
    "## 畳み込みオートエンコーダを使った画像の再構成\n",
    "**オートエンコーダ** とは, 入力次元を圧縮して **潜在表現** を学習し, その潜在表現から元のデータを復元するニューラルネットワークの1種である.  \n",
    "このオートエンコーダのアーキテクチャに畳み込みを使ったものが **畳み込みオートエンコーダ** である."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0135b69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 親クラスのコンストラクタを呼び出す\n",
    "        super(CNNEncoder, self).__init__()\n",
    "\n",
    "        # 畳み込み層の定義\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        # 活性化関数の定義\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        順伝播を行う関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            入力データ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        z: torch.Tensor\n",
    "            潜在表現データ\n",
    "        \"\"\"\n",
    "        # 1層目の計算\n",
    "        h = self.relu(self.conv1(x))\n",
    "\n",
    "        # 2層目の計算\n",
    "        h = self.relu(self.conv2(h))\n",
    "\n",
    "        # 3層目の計算\n",
    "        h = self.relu(self.conv3(h))\n",
    "\n",
    "        # 4層目の計算\n",
    "        z = self.relu(self.conv4(h))\n",
    "\n",
    "        return z\n",
    "\n",
    "\n",
    "class CNNDecoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 親クラスのコンストラクタを呼び出す\n",
    "        super(CNNDecoder, self).__init__()\n",
    "\n",
    "        # 転置畳み込み層の定義\n",
    "        self.deconv1 = nn.ConvTranspose2d(\n",
    "            512, 256, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.deconv2 = nn.ConvTranspose2d(\n",
    "            256, 128, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.deconv3 = nn.ConvTranspose2d(\n",
    "            128, 64, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "        self.deconv4 = nn.ConvTranspose2d(\n",
    "            64, 3, kernel_size=3, stride=2, padding=1, output_padding=1\n",
    "        )\n",
    "\n",
    "        # 活性化関数の定義\n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        順伝播を行う関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        z: torch.Tensor\n",
    "            潜在表現データ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        out: torch.Tensor\n",
    "            出力データ\n",
    "        \"\"\"\n",
    "        # 1層目の計算\n",
    "        h = self.relu(self.deconv1(z))\n",
    "\n",
    "        # 2層目の計算\n",
    "        h = self.relu(self.deconv2(h))\n",
    "\n",
    "        # 3層目の計算\n",
    "        h = self.relu(self.deconv3(h))\n",
    "\n",
    "        # 4層目の計算\n",
    "        out = self.sigmoid(self.deconv4(h))\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class CNNAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        # 親クラスのコンストラクタを呼び出す\n",
    "        super(CNNAutoencoder, self).__init__()\n",
    "\n",
    "        # エンコーダとデコーダの定義\n",
    "        self.encoder = CNNEncoder()\n",
    "        self.decoder = CNNDecoder()\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        順伝播を行う関数\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: torch.Tensor\n",
    "            入力データ\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        out: torch.Tensor\n",
    "            出力データ\n",
    "        \"\"\"\n",
    "        # エンコーダの計算\n",
    "        z = self.encoder(x)\n",
    "\n",
    "        # デコーダの計算\n",
    "        out = self.decoder(z)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19ca4ce",
   "metadata": {},
   "source": [
    "モデルのアーキテクチャを確認する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ced86dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルのインスタンスを作成\n",
    "model = CNNAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5a669d",
   "metadata": {},
   "source": [
    "学習のハイパーパラメータ等を設定する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c06963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 最適化手法の設定\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# 損失関数の設定\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# エポック数の設定\n",
    "num_epochs = 200\n",
    "\n",
    "# 学習履歴を保存するリストの初期化\n",
    "train_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48f1cf4",
   "metadata": {},
   "source": [
    "データを学習する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d00edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 再構成画像の表示回数の設定\n",
    "show_num = 10\n",
    "interval = num_epochs // show_num\n",
    "\n",
    "# 学習ループの実行\n",
    "with tqdm(range(num_epochs)) as pbar_epoch:\n",
    "    # 最初にモデルの出力を取得\n",
    "    outputs = outputs_setter(model, dataloader)\n",
    "\n",
    "    # 表示する画像の初期設定\n",
    "    _, axes = plt.subplots(2 + show_num, 8, figsize=(12, 2 * (2 + show_num)))\n",
    "    for i in range(8):\n",
    "        axes[0, i].imshow(images[i].permute(1, 2, 0))\n",
    "        axes[0, i].axis(\"off\")\n",
    "        axes[1, i].imshow(outputs[i].permute(1, 2, 0))\n",
    "        axes[1, i].axis(\"off\")\n",
    "\n",
    "    # エポックごとのループ\n",
    "    for epoch in pbar_epoch:\n",
    "        # エポック数の表示\n",
    "        pbar_epoch.set_description(f\"Epoch {epoch + 1}\")\n",
    "\n",
    "        # モデルを学習モードに設定\n",
    "        model.train()\n",
    "\n",
    "        # エポックの損失を初期化\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # ミニバッチ学習の実行\n",
    "        for images, _ in dataloader:\n",
    "            # 勾配の初期化\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 順伝播の計算\n",
    "            with torch.set_grad_enabled(True):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, images)\n",
    "\n",
    "                # 逆伝播の計算とパラメータの更新\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            # エポックの損失を更新\n",
    "            epoch_loss += loss.item() * images.size(0)\n",
    "\n",
    "        # エポックの平均損失を計算\n",
    "        epoch_loss /= len(dataloader.dataset)\n",
    "        train_loss_list.append(epoch_loss)\n",
    "\n",
    "        # 進捗バーに損失を表示\n",
    "        pbar_epoch.set_postfix(loss=epoch_loss)\n",
    "\n",
    "        # 再構成画像を表示\n",
    "        if (epoch + 1) % interval == 0:\n",
    "            # モデルの出力を取得\n",
    "            outputs = outputs_setter(model, dataloader)\n",
    "\n",
    "            # 元画像と再構成画像をプロット\n",
    "            for i in range(8):\n",
    "                axes[(epoch + 1) // interval + 1, i].imshow(\n",
    "                    outputs[i].permute(1, 2, 0)\n",
    "                )\n",
    "                axes[(epoch + 1) // interval + 1, i].axis(\"off\")\n",
    "\n",
    "    # 画像を表示\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6ff3c8",
   "metadata": {},
   "source": [
    "エポックを重ねるごとに再構成の精度が上がっていることが確認できる.  \n",
    "ただし, 60エポックくらいを過ぎたところから, もう大分元画像に近いものが生成できているため, それ以降の違いはあまり無いようにも受け止められる.  \n",
    "ロスの確認を行う."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcaef06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習ロス曲線を描画\n",
    "plot_data(train_loss_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5247fa6e",
   "metadata": {},
   "source": [
    "大体ロスも収束し, このオートエンコーダの生成が上手くいっている事がわかる."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17882814",
   "metadata": {},
   "source": [
    "## 潜在表現空間の可視化\n",
    "オートエンコーダの中で, エンコーダの出力する潜在表現をベクトルとして可視化することを考える."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f6775c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 潜在表現と画像データの取得\n",
    "latents = []\n",
    "images = []\n",
    "for imgs, _ in dataloader:\n",
    "    z = model.encoder(imgs)\n",
    "    latents.append(z.view(z.size(0), -1).numpy())\n",
    "    images.append(imgs)\n",
    "\n",
    "# 潜在表現をまとめる\n",
    "latents = np.concatenate(latents, axis=0)\n",
    "\n",
    "# 画像データをまとめる\n",
    "images = torch.cat(images, dim=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c8b343",
   "metadata": {},
   "source": [
    "上記で取得したエンコーダの潜在表現ベクトルは高次元すぎるため, **PCA(主成分分析)** によってノイズになる次元を削除し, **t-SNE** で非線形圧縮をして3次元に変換する."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a479d186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCAによる次元削減\n",
    "pca = PCA(n_components=50)\n",
    "latents_pca = pca.fit_transform(latents)\n",
    "\n",
    "# t-SNEによる次元削減\n",
    "tsne = TSNE(n_components=3, random_state=seed)\n",
    "latents_tsne = tsne.fit_transform(latents_pca)\n",
    "print(latents_tsne.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
